#ifndef __FASTCRC32C_ARM64__
#define __FASTCRC32C_ARM64__

/* Generated by https://github.com/corsix/fast-crc32/ using: */
/* ./generate -i neon_eor3 -p crc32c -a v12 */
/* MIT licensed */

#include <stddef.h>
#include <stdint.h>
#include <arm_acle.h>
#include <arm_neon.h>

#if defined(_MSC_VER)
#define CRC_AINLINE static __forceinline
#define CRC_ALIGN(n) __declspec(align(n))
#else
#define CRC_AINLINE static __inline __attribute__((always_inline))
#define CRC_ALIGN(n) __attribute__((aligned(n)))
#endif
#define CRC_EXPORT extern

CRC_AINLINE uint64x2_t clmul_lo(uint64x2_t a, uint64x2_t b) {
  uint64x2_t r;
  __asm("pmull %0.1q, %1.1d, %2.1d\n" : "=w"(r) : "w"(a), "w"(b));
  return r;
}

CRC_AINLINE uint64x2_t clmul_hi(uint64x2_t a, uint64x2_t b) {
  uint64x2_t r;
  __asm("pmull2 %0.1q, %1.2d, %2.2d\n" : "=w"(r) : "w"(a), "w"(b));
  return r;
}

CRC_EXPORT uint32_t crc32_impl(uint32_t crc0, const unsigned char* buf, size_t len) {
  crc0 = ~crc0;
  for (; len && ((uintptr_t)buf & 7); --len) {
    crc0 = __crc32cb(crc0, *buf++);
  }
  if (((uintptr_t)buf & 8) && len >= 8) {
    crc0 = __crc32cd(crc0, *(const uint64_t*)buf);
    buf += 8;
    len -= 8;
  }
  if (len >= 192) {
    /* First vector chunk. */
    uint64x2_t x0 = vld1q_u64((const uint64_t*)buf), y0;
    uint64x2_t x1 = vld1q_u64((const uint64_t*)(buf + 16)), y1;
    uint64x2_t x2 = vld1q_u64((const uint64_t*)(buf + 32)), y2;
    uint64x2_t x3 = vld1q_u64((const uint64_t*)(buf + 48)), y3;
    uint64x2_t x4 = vld1q_u64((const uint64_t*)(buf + 64)), y4;
    uint64x2_t x5 = vld1q_u64((const uint64_t*)(buf + 80)), y5;
    uint64x2_t x6 = vld1q_u64((const uint64_t*)(buf + 96)), y6;
    uint64x2_t x7 = vld1q_u64((const uint64_t*)(buf + 112)), y7;
    uint64x2_t x8 = vld1q_u64((const uint64_t*)(buf + 128)), y8;
    uint64x2_t x9 = vld1q_u64((const uint64_t*)(buf + 144)), y9;
    uint64x2_t x10 = vld1q_u64((const uint64_t*)(buf + 160)), y10;
    uint64x2_t x11 = vld1q_u64((const uint64_t*)(buf + 176)), y11;
    uint64x2_t k;
    { static const uint64_t CRC_ALIGN(16) k_[] = {0xa87ab8a8, 0xab7aff2a}; k = vld1q_u64(k_); }
    x0 = veorq_u64((uint64x2_t){crc0, 0}, x0);
    buf += 192;
    len -= 192;
    /* Main loop. */
    while (len >= 192) {
      y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
      y1 = clmul_lo(x1, k), x1 = clmul_hi(x1, k);
      y2 = clmul_lo(x2, k), x2 = clmul_hi(x2, k);
      y3 = clmul_lo(x3, k), x3 = clmul_hi(x3, k);
      y4 = clmul_lo(x4, k), x4 = clmul_hi(x4, k);
      y5 = clmul_lo(x5, k), x5 = clmul_hi(x5, k);
      y6 = clmul_lo(x6, k), x6 = clmul_hi(x6, k);
      y7 = clmul_lo(x7, k), x7 = clmul_hi(x7, k);
      y8 = clmul_lo(x8, k), x8 = clmul_hi(x8, k);
      y9 = clmul_lo(x9, k), x9 = clmul_hi(x9, k);
      y10 = clmul_lo(x10, k), x10 = clmul_hi(x10, k);
      y11 = clmul_lo(x11, k), x11 = clmul_hi(x11, k);
      x0 = veor3q_u64(x0, y0, vld1q_u64((const uint64_t*)buf));
      x1 = veor3q_u64(x1, y1, vld1q_u64((const uint64_t*)(buf + 16)));
      x2 = veor3q_u64(x2, y2, vld1q_u64((const uint64_t*)(buf + 32)));
      x3 = veor3q_u64(x3, y3, vld1q_u64((const uint64_t*)(buf + 48)));
      x4 = veor3q_u64(x4, y4, vld1q_u64((const uint64_t*)(buf + 64)));
      x5 = veor3q_u64(x5, y5, vld1q_u64((const uint64_t*)(buf + 80)));
      x6 = veor3q_u64(x6, y6, vld1q_u64((const uint64_t*)(buf + 96)));
      x7 = veor3q_u64(x7, y7, vld1q_u64((const uint64_t*)(buf + 112)));
      x8 = veor3q_u64(x8, y8, vld1q_u64((const uint64_t*)(buf + 128)));
      x9 = veor3q_u64(x9, y9, vld1q_u64((const uint64_t*)(buf + 144)));
      x10 = veor3q_u64(x10, y10, vld1q_u64((const uint64_t*)(buf + 160)));
      x11 = veor3q_u64(x11, y11, vld1q_u64((const uint64_t*)(buf + 176)));
      buf += 192;
      len -= 192;
    }
    /* Reduce x0 ... x11 to just x0. */
    { static const uint64_t CRC_ALIGN(16) k_[] = {0xf20c0dfe, 0x493c7d27}; k = vld1q_u64(k_); }
    y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
    y2 = clmul_lo(x2, k), x2 = clmul_hi(x2, k);
    y4 = clmul_lo(x4, k), x4 = clmul_hi(x4, k);
    y6 = clmul_lo(x6, k), x6 = clmul_hi(x6, k);
    y8 = clmul_lo(x8, k), x8 = clmul_hi(x8, k);
    y10 = clmul_lo(x10, k), x10 = clmul_hi(x10, k);
    x0 = veor3q_u64(x0, y0, x1);
    x2 = veor3q_u64(x2, y2, x3);
    x4 = veor3q_u64(x4, y4, x5);
    x6 = veor3q_u64(x6, y6, x7);
    x8 = veor3q_u64(x8, y8, x9);
    x10 = veor3q_u64(x10, y10, x11);
    { static const uint64_t CRC_ALIGN(16) k_[] = {0x3da6d0cb, 0xba4fc28e}; k = vld1q_u64(k_); }
    y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
    y4 = clmul_lo(x4, k), x4 = clmul_hi(x4, k);
    y8 = clmul_lo(x8, k), x8 = clmul_hi(x8, k);
    x0 = veor3q_u64(x0, y0, x2);
    x4 = veor3q_u64(x4, y4, x6);
    x8 = veor3q_u64(x8, y8, x10);
    { static const uint64_t CRC_ALIGN(16) k_[] = {0x740eef02, 0x9e4addf8}; k = vld1q_u64(k_); }
    y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
    x0 = veor3q_u64(x0, y0, x4);
    x4 = x8;
    y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
    x0 = veor3q_u64(x0, y0, x4);
    /* Reduce 128 bits to 32 bits, and multiply by x^32. */
    crc0 = __crc32cd(0, vgetq_lane_u64(x0, 0));
    crc0 = __crc32cd(crc0, vgetq_lane_u64(x0, 1));
  }
  for (; len >= 8; buf += 8, len -= 8) {
    crc0 = __crc32cd(crc0, *(const uint64_t*)buf);
  }
  for (; len; --len) {
    crc0 = __crc32cb(crc0, *buf++);
  }
  return ~crc0;
}


#endif